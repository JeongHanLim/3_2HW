{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Homework 1 (Due: Mar. 17, 2021 (11:59 PM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Lim Jeonghan(임정한)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student ID: 2017312172\n",
    "Late submission Days used here: 0\n",
    "\n",
    "[Please SUBMIT (1) YOUR IPYNB AND (2) PDF (please use FILE/DOWNLOAD AS/PDF or PRINT PREVIEW/PRINT AS PDF with \"printed output\") TO iCampus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "For this homework you cannot use the python library scikit-learn (sklearn). \n",
    "You can use the python package BeautifulSoup to parse web pages.\n",
    "\n",
    "In this assignment you will retrieve and parse webpages. The text file \"urls.txt\" contains a list of urls for the webpages to be parsed. Each line in the text file corresponds to a url. Use BeautifulSoup to fetch each webpage.\n",
    "\n",
    "Note: For all questions, the words should be converted to lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Q1: Part 1 (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Parse the first webpage document to retrieve the text enclosed in paragraph tags, find the words that end in \"ing\", and count how many times each word appears. \n",
    "\n",
    "Sort these words in decreasing order of frequency, and write the words (along with their corresponding frequencies) in an output file named \"Q1_Part1.txt\". The most frequent word should appear at the top and the least frequent word at the end, and the format of the output file should be:\n",
    "word TAB frequency\n",
    "\n",
    "Example:\n",
    "\n",
    "sorting\t10\n",
    "training\t8\n",
    "broadening\t6\n",
    "extracting\t3\n",
    "evergrowing\t2\n",
    "coming\t1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning \t 5\n",
      "mining \t 3\n",
      "beijing \t 2\n",
      "computing \t 2\n",
      "accounting \t 1\n",
      "analyzing \t 1\n",
      "applying \t 1\n",
      "becoming \t 1\n",
      "being \t 1\n",
      "breaking \t 1\n",
      "changing \t 1\n",
      "combining \t 1\n",
      "creating \t 1\n",
      "describing \t 1\n",
      "developing \t 1\n",
      "drawing \t 1\n",
      "during \t 1\n",
      "emerging \t 1\n",
      "enabling \t 1\n",
      "everything \t 1\n",
      "extracting \t 1\n",
      "finding \t 1\n",
      "formulating \t 1\n",
      "growing \t 1\n",
      "including \t 1\n",
      "managing \t 1\n",
      "preparing \t 1\n",
      "presenting \t 1\n",
      "reflecting \t 1\n",
      "training \t 1\n",
      "turing \t 1\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/Data_science\"\n",
    "\n",
    "def get_text(URL):\n",
    "    source_code_from_URL = urllib.request.urlopen(URL)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
    "    return ''.join([str(item.find_all(text=True)) for item in soup.find_all('p')])\n",
    "\n",
    "def split_text(text):\n",
    "    return text.split(' ')\n",
    "\n",
    "def extract_keyword(words, target):\n",
    "    keyword = []\n",
    "    for word in words:\n",
    "        if len(word.split(target))>=2:\n",
    "            if not word.split(target)[1].isalpha():\n",
    "                keyword.append(word)\n",
    "\n",
    "    return keyword\n",
    "\n",
    "def delete_stop_words(words):\n",
    "    f = open(\"stop_words.txt\", 'r')\n",
    "    stop_word = []\n",
    "    returningwords= []\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        stop_word.append(line.rstrip())\n",
    "    f.close()\n",
    "\n",
    "    for word in words:\n",
    "        if word not in stop_word:\n",
    "            print(word)\n",
    "            returningwords.append(word)\n",
    "    return returningwords\n",
    "\n",
    "\n",
    "def clean_nonalpha(texts):\n",
    "    text = texts.replace('\\\\n', \"\")\n",
    "    text = re.sub(pattern=\"[^\\w\\s]\", repl=\"\", string=text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def lower_words(text):\n",
    "    return text.lower()\n",
    "\n",
    "def make_dict(words):\n",
    "    frequency = {}\n",
    "    for word in words:\n",
    "        if word != \" \":\n",
    "            count = frequency.get(word, 0)\n",
    "            frequency[word] = count + 1\n",
    "\n",
    "    sorted_frequency = sorted(frequency.items(), key = lambda x:x[0])\n",
    "    sorted_frequency = sorted(sorted_frequency, reverse=True, key=lambda x: x[1])\n",
    "\n",
    "    return sorted_frequency\n",
    "\n",
    "def save_txt(dictionary):\n",
    "    with open(\"Q1_Part1.txt\", \"a+\") as f:\n",
    "        for x in dictionary:\n",
    "            f.write(str(x[0])+\"\\t\"+str(x[1])+\"\\n\")\n",
    "\n",
    "def main():\n",
    "\n",
    "    result_text = get_text(URL)\n",
    "    words = result_text.lower()\n",
    "    words = clean_nonalpha(words)\n",
    "    words = split_text(words)\n",
    "\n",
    "    keyword = extract_keyword(words, \"ing\")\n",
    "    frequency = make_dict(keyword)\n",
    "\n",
    "\n",
    "    for x in frequency:\n",
    "        print(x[0],\"\\t\", x[1])\n",
    "    save_txt(frequency)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Q1: Part 2 (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Stop words are natural language words which have very little meaning, such as \"and\", \"the\", \"a\", \"an\", and similar words.\n",
    "\n",
    "Repeat Part 1, but before counting, remove the stop words given in the file \"stop_words.txt\". The ouput for Part 2 should have the same format as Part 1, and should be written to an output file named \"Q1_Part2.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning  :: 5\n",
      "mining  :: 3\n",
      "beijing  :: 2\n",
      "computing  :: 2\n",
      "accounting  :: 1\n",
      "analyzing  :: 1\n",
      "applying  :: 1\n",
      "breaking  :: 1\n",
      "changing  :: 1\n",
      "combining  :: 1\n",
      "creating  :: 1\n",
      "describing  :: 1\n",
      "developing  :: 1\n",
      "drawing  :: 1\n",
      "emerging  :: 1\n",
      "enabling  :: 1\n",
      "extracting  :: 1\n",
      "finding  :: 1\n",
      "formulating  :: 1\n",
      "growing  :: 1\n",
      "including  :: 1\n",
      "managing  :: 1\n",
      "preparing  :: 1\n",
      "presenting  :: 1\n",
      "reflecting  :: 1\n",
      "training  :: 1\n",
      "turing  :: 1\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/Data_science\"\n",
    "\n",
    "def get_text(URL):\n",
    "    source_code_from_URL = urllib.request.urlopen(URL)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
    "    return ''.join([str(item.find_all(text=True)) for item in soup.find_all('p')])\n",
    "\n",
    "\n",
    "def split_text(text):\n",
    "    return text.split(' ')\n",
    "\n",
    "\n",
    "def clean_nonalpha(text):\n",
    "    text = text.replace('\\\\n', \"\")\n",
    "    text = re.sub(pattern=\"[^\\w\\s]\", repl=\"\", string=text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def lower_words(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def make_dict(words):\n",
    "    frequency = {}\n",
    "    for word in words:\n",
    "        count = frequency.get(word, 0)\n",
    "        frequency[word] = count + 1\n",
    "\n",
    "    sorted_frequency = sorted(frequency.items(), key = lambda x: x[0])\n",
    "    sorted_frequency = sorted(sorted_frequency, reverse = True, key = lambda x: x[1])\n",
    "\n",
    "    return sorted_frequency\n",
    "\n",
    "\n",
    "def delete_stop_words(words):\n",
    "    f = open(\"stop_words.txt\", 'r')\n",
    "    stop_word = []\n",
    "    returningwords= []\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        stop_word.append(line.rstrip())\n",
    "    f.close()\n",
    "\n",
    "    for word in words:\n",
    "        if word not in stop_word:\n",
    "            returningwords.append(word)\n",
    "    return returningwords\n",
    "\n",
    "\n",
    "def extract_keyword(words, target):\n",
    "    keyword = []\n",
    "    for word in words:\n",
    "        if len(word.split(target))>=2:\n",
    "            if not word.split(target)[1].isalpha():\n",
    "                keyword.append(word)\n",
    "\n",
    "    return keyword\n",
    "def save_txt(dictionary):\n",
    "    with open(\"Q1_Part2.txt\", \"a+\") as f:\n",
    "        for x in dictionary:\n",
    "            f.write(str(x[0])+\"\\t\"+str(x[1])+\"\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    result_text = get_text(URL)\n",
    "    words = result_text.lower()\n",
    "    words = clean_nonalpha(words)\n",
    "    words = split_text(words)\n",
    "    words = delete_stop_words(words)\n",
    "    keyword = extract_keyword(words, \"ing\")\n",
    "    frequency = make_dict(keyword)\n",
    "\n",
    "    for x in frequency:\n",
    "        print(x[0], \" ::\", x[1])\n",
    "    save_txt(frequency)\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 (10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Again, parse the first webpage document, but this time find and count all outgoing links to other webpages, and write the output to a file named \"Q2.txt\", with each url on a new line.\n",
    "\n",
    "The format of the output file should: number of outgoing urls in the first line, followed by each url on a new line. \n",
    "For example, \n",
    "4\n",
    "https://eng.skku.edu/eng/edu/education.do\n",
    "https://eng.skku.edu/eng/Research/industry/researchStory.do\n",
    "https://eng.skku.edu/eng/Univ-Industry/Research-Business-Found/FactsandFigures.do\n",
    "https://eng.skku.edu/eng/CampusLife/support/employment.do\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/list/cs.LG/recent\n",
      "https://en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&action=edit\n",
      "https://en.wikipedia.org/w/index.php?title=Data_science&action=edit\n",
      "http://cacm.acm.org/magazines/2013/12/169933-data-science-and-prediction/fulltext\n",
      "https://doi.org/10.1145%2F2500499\n",
      "https://api.semanticscholar.org/CorpusID:6107147\n",
      "https://web.archive.org/web/20141109113411/http://cacm.acm.org/magazines/2013/12/169933-data-science-and-prediction/fulltext\n",
      "http://simplystatistics.org/2013/12/12/the-key-word-in-data-science-is-not-data-it-is-science/\n",
      "https://web.archive.org/web/20140102194117/http://simplystatistics.org/2013/12/12/the-key-word-in-data-science-is-not-data-it-is-science/\n",
      "https://www.springer.com/book/9784431702085\n",
      "https://doi.org/10.1007%2F978-4-431-65950-1_3\n",
      "https://books.google.com/books?id=oGs_AQAAIAAJ\n",
      "https://web.archive.org/web/20170320193019/https://books.google.com/books?id=oGs_AQAAIAAJ\n",
      "https://doi.org/10.1126%2Fscience.1170411\n",
      "https://api.semanticscholar.org/CorpusID:9743327\n",
      "http://www.datascienceassn.org/about-data-science\n",
      "https://www.oreilly.com/library/view/doing-data-science/9781449363871/ch01.html\n",
      "https://medriscoll.com/post/4740157098/the-three-sexy-skills-of-data-geeks\n",
      "https://flowingdata.com/2009/06/04/rise-of-the-data-scientist/\n",
      "https://benfry.com/phd/dissertation/2.html\n",
      "https://magazine.amstat.org/blog/2015/10/01/asa-statement-on-the-role-of-statistics-in-data-science/\n",
      "https://web.archive.org/web/20190620184935/https://magazine.amstat.org/blog/2015/10/01/asa-statement-on-the-role-of-statistics-in-data-science/\n",
      "https://www.statisticsviews.com/article/nate-silver-what-i-need-from-statisticians/\n",
      "http://priceonomics.com/whats-the-difference-between-data-science-and/\n",
      "https://statmodeling.stat.columbia.edu/2013/11/14/statistics-least-important-part-data-science/\n",
      "https://www.datasciencecentral.com/profiles/blogs/data-science-without-statistics-is-possible-even-desirable\n",
      "http://courses.csail.mit.edu/18.337/2015/docs/50YearsDataScience.pdf\n",
      "https://www2.isye.gatech.edu/~jeffwu/publications/fazhan.pdf\n",
      "https://www.mdpi.com/2504-2289/2/2/14\n",
      "https://doi.org/10.3390%2Fbdcc2020014\n",
      "https://doi.org/10.1145%2F3076253\n",
      "http://www2.isye.gatech.edu/~jeffwu/presentations/datascience.pdf\n",
      "https://www.forbes.com/sites/gilpress/2013/05/28/a-very-short-history-of-data-science/\n",
      "https://www.stat.purdue.edu/~wsc/\n",
      "https://magazine.amstat.org/blog/2016/06/01/datascience-2/\n",
      "https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century\n",
      "https://www.nsf.gov/pubs/2005/nsb0540/\n",
      "https://www.forbes.com/sites/gilpress/2013/08/19/data-science-whats-the-half-life-of-a-buzzword/\n",
      "https://www.forbes.com/sites/peterpham/2015/08/28/the-impacts-of-big-data-that-you-may-not-have-heard-of/\n",
      "https://towardsdatascience.com/how-data-science-will-impact-future-of-businesses-7f11f5699c4d\n",
      "https://sites.engineering.ucsb.edu/~shell/che210d/python.pdf\n",
      "https://cran.r-project.org/doc/FAQ/R-FAQ.html#What-is-R_003f\n",
      "https://www.wired.com/2014/07/a-drag-and-drop-toolkit-that-lets-anyone-create-interactive-maps/\n",
      "https://en.wikipedia.org/w/index.php?title=Template:Data&action=edit\n",
      "https://en.wikipedia.org/w/index.php?title=Data_science&oldid=1011117061\n",
      "https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q2374463\n",
      "https://commons.wikimedia.org/wiki/Category:Data_science\n",
      "https://ar.wikipedia.org/wiki/%D8%B9%D9%84%D9%85_%D8%A7%D9%84%D8%A8%D9%8A%D8%A7%D9%86%D8%A7%D8%AA\n",
      "https://az.wikipedia.org/wiki/Veril%C9%99nl%C9%99r_elmi_(Data_Science)\n",
      "https://bn.wikipedia.org/wiki/%E0%A6%89%E0%A6%AA%E0%A6%BE%E0%A6%A4%E0%A7%8D%E0%A6%A4_%E0%A6%AC%E0%A6%BF%E0%A6%9C%E0%A7%8D%E0%A6%9E%E0%A6%BE%E0%A6%A8\n",
      "https://ca.wikipedia.org/wiki/Ci%C3%A8ncia_de_les_dades\n",
      "https://cs.wikipedia.org/wiki/Data_science\n",
      "https://de.wikipedia.org/wiki/Data_Science\n",
      "https://et.wikipedia.org/wiki/Andmeteadus\n",
      "https://el.wikipedia.org/wiki/%CE%95%CF%80%CE%B9%CF%83%CF%84%CE%AE%CE%BC%CE%B7_%CE%B4%CE%B5%CE%B4%CE%BF%CE%BC%CE%AD%CE%BD%CF%89%CE%BD\n",
      "https://es.wikipedia.org/wiki/Ciencia_de_datos\n",
      "https://eu.wikipedia.org/wiki/Datu_zientzia\n",
      "https://fa.wikipedia.org/wiki/%D8%B9%D9%84%D9%85_%D8%AF%D8%A7%D8%AF%D9%87%E2%80%8C%D9%87%D8%A7\n",
      "https://fr.wikipedia.org/wiki/Science_des_donn%C3%A9es\n",
      "https://ko.wikipedia.org/wiki/%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4\n",
      "https://hy.wikipedia.org/wiki/%D5%8F%D5%BE%D5%B5%D5%A1%D5%AC%D5%B6%D5%A5%D6%80%D5%AB_%D5%A3%D5%AB%D5%BF%D5%B8%D6%82%D5%A9%D5%B5%D5%B8%D6%82%D5%B6\n",
      "https://hi.wikipedia.org/wiki/%E0%A4%86%E0%A4%81%E0%A4%95%E0%A4%A1%E0%A4%BC%E0%A4%BE_%E0%A4%B5%E0%A4%BF%E0%A4%9C%E0%A5%8D%E0%A4%9E%E0%A4%BE%E0%A4%A8\n",
      "https://id.wikipedia.org/wiki/Ilmu_data\n",
      "https://it.wikipedia.org/wiki/Scienza_dei_dati\n",
      "https://he.wikipedia.org/wiki/%D7%9E%D7%93%D7%A2_%D7%94%D7%A0%D7%AA%D7%95%D7%A0%D7%99%D7%9D\n",
      "https://kk.wikipedia.org/wiki/%D0%94%D0%B5%D1%80%D0%B5%D0%BA%D1%82%D0%B5%D1%80_%D1%82%D1%83%D1%80%D0%B0%D0%BB%D1%8B_%D2%93%D1%8B%D0%BB%D1%8B%D0%BC\n",
      "https://lv.wikipedia.org/wiki/Datu_m%C4%81c%C4%ABba\n",
      "https://mk.wikipedia.org/wiki/%D0%9D%D0%B0%D1%83%D0%BA%D0%B0_%D0%B7%D0%B0_%D0%BF%D0%BE%D0%B4%D0%B0%D1%82%D0%BE%D1%86%D0%B8\n",
      "https://ms.wikipedia.org/wiki/Sains_data\n",
      "https://my.wikipedia.org/wiki/%E1%80%A1%E1%80%81%E1%80%BB%E1%80%80%E1%80%BA%E1%80%A1%E1%80%9C%E1%80%80%E1%80%BA%E1%80%9E%E1%80%AD%E1%80%95%E1%80%B9%E1%80%95%E1%80%B6%E1%80%95%E1%80%8A%E1%80%AC\n",
      "https://nl.wikipedia.org/wiki/Datawetenschap\n",
      "https://ja.wikipedia.org/wiki/%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%82%B9\n",
      "https://pl.wikipedia.org/wiki/Danologia\n",
      "https://pt.wikipedia.org/wiki/Ci%C3%AAncia_de_dados\n",
      "https://ru.wikipedia.org/wiki/%D0%9D%D0%B0%D1%83%D0%BA%D0%B0_%D0%BE_%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85\n",
      "https://simple.wikipedia.org/wiki/Data_science\n",
      "https://fi.wikipedia.org/wiki/Datatiede\n",
      "https://ta.wikipedia.org/wiki/%E0%AE%A4%E0%AE%B0%E0%AE%B5%E0%AF%81_%E0%AE%85%E0%AE%B1%E0%AE%BF%E0%AE%B5%E0%AE%BF%E0%AE%AF%E0%AE%B2%E0%AF%8D\n",
      "https://th.wikipedia.org/wiki/%E0%B8%A7%E0%B8%B4%E0%B8%97%E0%B8%A2%E0%B8%B2%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%A1%E0%B8%B9%E0%B8%A5\n",
      "https://tr.wikipedia.org/wiki/Veri_bilimi\n",
      "https://uk.wikipedia.org/wiki/%D0%9D%D0%B0%D1%83%D0%BA%D0%B0_%D0%BF%D1%80%D0%BE_%D0%B4%D0%B0%D0%BD%D1%96\n",
      "https://ur.wikipedia.org/wiki/%DA%88%DB%8C%D9%B9%D8%A7_%D8%B3%D8%A7%D8%A6%D9%86%D8%B3\n",
      "https://vi.wikipedia.org/wiki/Khoa_h%E1%BB%8Dc_d%E1%BB%AF_li%E1%BB%87u\n",
      "https://zh-yue.wikipedia.org/wiki/%E6%95%B8%E6%93%9A%E7%A7%91%E5%AD%B8\n",
      "https://zh.wikipedia.org/wiki/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q2374463#sitelinks-wikipedia\n",
      "https://foundation.wikimedia.org/wiki/Privacy_policy\n",
      "https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute\n",
      "https://stats.wikimedia.org/#/en.wikipedia.org\n",
      "https://foundation.wikimedia.org/wiki/Cookie_statement\n",
      "https://wikimediafoundation.org/\n",
      "https://www.mediawiki.org/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def getExternalLinks(bs):\n",
    "    externalLinks = []\n",
    "    for link in bs.find_all('a', {'href' : re.compile('^(http|www)(.)*$')}):\n",
    "        if link.attrs['href'] is not None:\n",
    "            if link.attrs['href'] not in externalLinks:\n",
    "                externalLinks.append(link.attrs['href'])\n",
    "    return externalLinks\n",
    "\n",
    "def save_text(links):\n",
    "    with open(\"Q2.txt\", \"a+\") as f:\n",
    "        for link in links:\n",
    "            f.write(link+\"\\n\")\n",
    "\n",
    "\n",
    "r = requests.get(\"http://en.wikipedia.org/wiki/Data_Science\")\n",
    "soup = BeautifulSoup(r.content)\n",
    "\n",
    "links = getExternalLinks(soup)\n",
    "for link in links:\n",
    "    print(link)\n",
    "\n",
    "save_text(links)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Q3: Part 1 (10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "1. Retrieve and parse multiple web pages. The text file \"urls.txt\" contains a list of webpages to be parsed. Each line in the text file corresponds to a url. Use BeautifulSoup to fetch each webpage and parse it as specified below. \n",
    "\n",
    "2. For each webpage document do the following:\n",
    "    1. Retrieve all text enclosed in paragraph tags. \n",
    "    2. Convert the text to lowercase. \n",
    "    3. Strip out punctuation. Note: if you use translate() with string.punctuation, then it may not strip out all characters. Use a regular expression involving \\W to strip out all non alpha-numeric characters.\n",
    "    4. Tokenize into words based on whitespace separation.\n",
    "\n",
    "3. Find the number of unique words in each webpage document. \n",
    "\n",
    "4. Find the Length of each webpage document. The length of a document is defined as the total number of words in the document (not just unique words).\n",
    "\n",
    "5. For each of the following words: “statistics”, “analytics”, “data”, and “science”, \n",
    "    a. Find Term Frequency (tf). \n",
    "    The term frequency (tf) of a term (word) is defined as the number of times that term t occurs in document d, \n",
    "    divided by the total number of words in the document. \n",
    "    The tf of a word depends on the document under consideration. \n",
    "    \n",
    "    b. Find Inverse Document Frequency (idf).\n",
    "    The inverse document frequency of a word is the logarithmically scaled inverse fraction of the documents that \n",
    "    contain the word, obtained by dividing the total number of documents by the number of documents containing the \n",
    "    term, and then taking the logarithm of that ratio.\n",
    "    The idf of a word doesn't depend on any documnet in which the word is present. \n",
    "    To calculate the idf, you will have to use the log function. The base for the log function must be e.\n",
    "    \n",
    "    c. Find tf-idf. \n",
    "    The tf-idf of a word is the product of the term frequency of the word in document d, and its inverse document \n",
    "    frequency. \n",
    "    The tf-idf of a word depends on the document under consideration. \n",
    "    \n",
    "    Reference: https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "    \n",
    "The output should be written to an output file named \"Q3_Part1.txt\".\n",
    "\n",
    "The format of the output file is as shown below:\n",
    "\n",
    "1. Number of unique words in documents: [702, 723, 280]\n",
    "\n",
    "2. Length of documents: [1711, 1928, 563]\n",
    "\n",
    "3. tf\n",
    "    statistics: [0.0070134424313267095, 0.0025933609958506223, 0.0]\n",
    "    analytics: [0.0029222676797194622, 0.0031120331950207467, 0.0]\n",
    "    data: [0.056107539450613676, 0.05446058091286307, 0.0]\n",
    "    science: [0.03798947983635301, 0.011410788381742738, 0.028419182948490232]\n",
    "    \n",
    "4. idf \n",
    "    statistics: 0.510825623766\n",
    "    analytics: 0.510825623766\n",
    "    data: 0.223143551314\n",
    "    science: 0.0\n",
    "    \n",
    "5. tf-idf\n",
    "    statistics: [0.0028437061936282715, 0.0010515173965460695, 0.0]\n",
    "    analytics: [0.0011848775806784465, 0.0012618208758552834, 0.0]\n",
    "    data: [0.022749649549026172, 0.022081865327467459, 0.0]\n",
    "    science: [0.0, 0.0, 0.0]\n",
    "   \n",
    "The above values are for the first three webpage urls given in the file \"urls.txt\". The number of unique words in documents, average length of documents, tf and tf-idf values for the four words, must be in the order of the urls given in \"urls.txt\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in documents: [483, 2129, 801, 1015, 1339]\n",
      "Length of documents [1034, 6154, 1937, 2230, 3581]\n",
      "tf statistics [0.01644100580270793, 0.00032499187520311994, 0.002581311306143521, 0, 0.002513264451270595]  analytics [0.0009671179883945841, 0.0029249268768280793, 0.002065049044914817, 0, 0.0005585032113934655]  data [0.05029013539651837, 0.04143646408839779, 0.04697986577181208, 0, 0.030996928232337337]  science [0.03288201160541586, 0.0008124796880077998, 0.011357769747031492, 0.0004484304932735426, 0.007539793353811784]  \n",
      "idf statistics 0.22314355131420976  analytics 0.22314355131420976  data 0.22314355131420976  science 0.0  \n",
      "tf-idf statistics [0.0036687044219937772, 7.251984118108865e-05, 0.0005760029719003866, 0.0, 0.0005608187550482793]  analytics [0.0002158061424702222, 0.0006526785706297978, 0.0004608023775203093, 0.0, 0.0001246263900107287]  data [0.011221919408451554, 0.009246279750588803, 0.010483254088587036, 0.0, 0.006916764645595444]  science [0.0, 0.0, 0.0, 0.0, 0.0]  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_url():\n",
    "    urlist = []\n",
    "    with open(\"urls.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            urlist.append(line.rstrip())\n",
    "    return urlist\n",
    "\n",
    "\n",
    "def get_text(URL, option = 'p'):\n",
    "    source_code_from_URL = urllib.request.urlopen(URL)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
    "    return ''.join([str(item.find_all(text=True)) for item in soup.find_all(option)])\n",
    "\n",
    "\n",
    "def lower_words(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def strip_punctuation_space(text):\n",
    "\n",
    "    text = text.replace('\\\\n', \"\")\n",
    "    text = re.sub(pattern = \"[^\\w\\s]\", repl = \"\", string = text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def make_dict(text):\n",
    "    frequency = {}\n",
    "\n",
    "    for word in text:\n",
    "        count = frequency.get(word, 0)\n",
    "        frequency[word] = count + 1\n",
    "    sorted_frequency = sorted(frequency.items(), reverse = False, key = lambda x: x[1])\n",
    "    len_word = len(sorted_frequency)\n",
    "    return len_word, sorted_frequency\n",
    "\n",
    "\n",
    "def tf(keyword, len_web, text):\n",
    "    keyword_dict = {}\n",
    "    for word in text:\n",
    "        count = keyword_dict.get(word, 0)\n",
    "        keyword_dict[word] = count + 1\n",
    "\n",
    "    tf = dict.fromkeys(keyword, 0)\n",
    "    for idx in keyword:\n",
    "        if idx in keyword_dict:\n",
    "            tf[idx]= (keyword_dict[idx]/len_web)\n",
    "    #tf = dict((k, keyword_dict[k]/len_web) for k in keyword if k in keyword_dict)\n",
    "    return tf\n",
    "\n",
    "\n",
    "\n",
    "def get_idf(dict, urls):\n",
    "    idf_list = {}\n",
    "\n",
    "    for word, tf_value in dict.items():\n",
    "        idf_list[word] = np.log(len(urls) / (len(urls) - tf_value.count(0)))\n",
    "    return idf_list\n",
    "\n",
    "\n",
    "def cal_tf_idf(tf, idf, keyword):\n",
    "    tf_idf = {}\n",
    "    for word, value in tf.items():\n",
    "        tf_idf[word] = np.array(value) * idf[word]\n",
    "        tf_idf[word] = list(tf_idf[word])\n",
    "    return tf_idf\n",
    "\n",
    "def print_dict(dict):\n",
    "    for key in dict.keys():\n",
    "        print(key, dict[key], \" \", end=\"\")\n",
    "    print(\"\\n\", end=\"\")\n",
    "\n",
    "def save_txt(lista, listb, dicta, dictb, dictc):\n",
    "    f = open(\"Q3_Part1.txt\", \"a+\")\n",
    "    f.write(\"Number of unique words in documents: \"+str(lista)+\"\\n\")\n",
    "    f.write(\"Length of documents: \"+str(listb)+\"\\n\")\n",
    "\n",
    "    f.write(\"tf \")\n",
    "    for key in dicta.keys():\n",
    "        f.write(str(key)+\" \"+str(dicta[key])+\"  \")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    f.write(\"idf \")\n",
    "    for key in dictb.keys():\n",
    "        f.write(str(key)+\" \" + str(dictb[key]) + \"  \")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    f.write(\"tf-idf \")\n",
    "    for key in dictc.keys():\n",
    "        f.write(str(key) +\" \"+ str(dictc[key]) + \" \")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    keyword = [\"statistics\", \"analytics\", \"data\", \"science\"]\n",
    "    urls = get_url()\n",
    "\n",
    "    n_words_list = []\n",
    "    len_document_list = []\n",
    "    tf_dict_all = defaultdict(list)\n",
    "    idf_list = dict([(k, 0) for k in keyword])\n",
    "    tf_idf_dict = {}\n",
    "    for URL in urls:\n",
    "\n",
    "        result_text = get_text(URL, option = \"p\")\n",
    "        result_text.lower()\n",
    "\n",
    "        # STRIPPING\n",
    "        result_text = strip_punctuation_space(result_text)\n",
    "        result_text = result_text.split(' ')\n",
    "\n",
    "        # GETTING DATA\n",
    "        len_web = len(result_text)\n",
    "        len_word, _ = make_dict(result_text)\n",
    "        tf_dict = tf(keyword, len_web, result_text)\n",
    "\n",
    "        # ADDING TO LIST\n",
    "        n_words_list.append(len_word)\n",
    "        len_document_list.append(len_web)\n",
    "\n",
    "        # MAKING DICTIONARY OF TF, TF-IDF\n",
    "        for word, tf_value in tf_dict.items():\n",
    "            tf_dict_all[word].append(tf_value)\n",
    "\n",
    "    idf_list = get_idf(tf_dict_all, urls)\n",
    "    tf_idf_dict = cal_tf_idf(tf_dict_all, idf_list, keyword)\n",
    "\n",
    "\n",
    "    print(\"Number of unique words in documents:\", n_words_list)\n",
    "    print(\"Length of documents\", len_document_list)\n",
    "    print(\"tf \", end=\"\")\n",
    "    print_dict(dict(tf_dict_all))\n",
    "    print(\"idf \", end=\"\")\n",
    "    print_dict(idf_list)\n",
    "    print(\"tf-idf \", end=\"\")\n",
    "    print_dict(tf_idf_dict)\n",
    "    save_txt(n_words_list, len_document_list, tf_dict_all, idf_list, tf_idf_dict)\n",
    "\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Q3: Part 2 (10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Repeat Part 1, but first remove the stop words given in the file \"stop_words.txt\".  \n",
    "The ouput for Part 2 should have the same format as Part 1, and should be written to an output file named \"Q3_Part2.txt\".\n",
    "Note: The length of document, in this case, will not include stop words. Similarly, the number of unique words in documents, and the calculation of tf, idf, tf-idf should be done after removing the stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Sample output for the file \"urls.txt\"\n",
    "\n",
    "1. Number of unique words in documents: [596, 600, 231]\n",
    "\n",
    "2. Length of documents: [1020, 1079, 357]  \n",
    "\n",
    "3. tf\n",
    "    statistics: [0.011764705882352941, 0.004633920296570899, 0.0]\n",
    "    analytics: [0.004901960784313725, 0.005560704355885079, 0.0]\n",
    "    data: [0.09411764705882353, 0.09731232622798888, 0.0]\n",
    "    science: [0.06372549019607843, 0.020389249304911955, 0.04481792717086835]\n",
    "    \n",
    "4. idf \n",
    "    statistics: 0.405465108108\n",
    "    analytics: 0.405465108108\n",
    "    data: 0.405465108108\n",
    "    science: 0.0\n",
    "    \n",
    "5. tf-idf\n",
    "    statistics: [0.0047701777424489925, 0.0018788929940137366, 0.0]\n",
    "    analytics: [0.0019875740593537469, 0.002254671592816484, 0.0]\n",
    "    data: [0.03816142193959194, 0.039456752874288473, 0.0]\n",
    "    science: [0.0, 0.0, 0.0] \n",
    "      \n",
    "The above values are for the first three webpage urls given in the file \"urls.txt\". The number of unique words in documents, average length of documents, tf and tf-idf values for the four words, must be in the order of the urls given in \"urls.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in documents: [405, 1958, 691, 921, 1195]\n",
      "Length of documents [661, 3876, 1172, 1460, 2146]\n",
      "tf statistics [0.025718608169440244, 0.0005159958720330237, 0.004266211604095563, 0, 0.004193849021435228]  analytics [0.0015128593040847202, 0.0046439628482972135, 0.0034129692832764505, 0, 0.0009319664492078285]  data [0.07866868381240545, 0.06578947368421052, 0.07764505119453925, 0, 0.05172413793103448]  science [0.05143721633888049, 0.0012899896800825593, 0.01877133105802048, 0.0006849315068493151, 0.012581547064305684]  \n",
      "idf statistics 0.22314355131420976  analytics 0.22314355131420976  data 0.22314355131420976  science 0.0  \n",
      "tf-idf statistics [0.005738941561787544, 0.00011514115134892143, 0.0009519776079957755, 0.0, 0.0009358303643186803]  analytics [0.0003375847977522084, 0.001036270362140293, 0.0007615820863966203, 0.0, 0.00020796230318192896]  data [0.01755440948311484, 0.014680496796987484, 0.017325992465523115, 0.0, 0.011541907826597057]  science [0.0, 0.0, 0.0, 0.0, 0.0]  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_url():\n",
    "    urlist = []\n",
    "    with open(\"urls.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            urlist.append(line.rstrip())\n",
    "    return urlist\n",
    "\n",
    "\n",
    "def get_text(URL, option = 'p'):\n",
    "    source_code_from_URL = urllib.request.urlopen(URL)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
    "    return ''.join([str(item.find_all(text=True)) for item in soup.find_all(option)])\n",
    "\n",
    "\n",
    "def lower_words(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def strip_punctuation_space(text):\n",
    "\n",
    "    text = text.replace('\\\\n', \"\")\n",
    "    text = re.sub(pattern = \"[^\\w\\s]\", repl = \"\", string = text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def make_dict(text):\n",
    "    frequency = {}\n",
    "\n",
    "    for word in text:\n",
    "        count = frequency.get(word, 0)\n",
    "        frequency[word] = count + 1\n",
    "    sorted_frequency = sorted(frequency.items(), reverse = False, key = lambda x: x[1])\n",
    "    len_word = len(sorted_frequency)\n",
    "    return len_word, sorted_frequency\n",
    "\n",
    "\n",
    "def tf(keyword, len_web, text):\n",
    "    keyword_dict = {}\n",
    "    for word in text:\n",
    "        count = keyword_dict.get(word, 0)\n",
    "        keyword_dict[word] = count + 1\n",
    "\n",
    "    tf = dict.fromkeys(keyword, 0)\n",
    "    for idx in keyword:\n",
    "        if idx in keyword_dict:\n",
    "            tf[idx]= (keyword_dict[idx]/len_web)\n",
    "    #tf = dict((k, keyword_dict[k]/len_web) for k in keyword if k in keyword_dict)\n",
    "    return tf\n",
    "\n",
    "def get_idf(dict, urls):\n",
    "    idf_list = {}\n",
    "\n",
    "    for word, tf_value in dict.items():\n",
    "        idf_list[word] = np.log(len(urls) / (len(urls) - tf_value.count(0)))\n",
    "    return idf_list\n",
    "\n",
    "def cal_tf_idf(tf, idf, keyword):\n",
    "    tf_idf = {}\n",
    "    for word, value in tf.items():\n",
    "        tf_idf[word] = np.array(value) * idf[word]\n",
    "        tf_idf[word] = list(tf_idf[word])\n",
    "    return tf_idf\n",
    "\n",
    "\n",
    "def delete_stop_words(words):\n",
    "    f = open(\"stop_words.txt\", 'r')\n",
    "    stop_word = []\n",
    "    returningwords= []\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        stop_word.append(line.rstrip())\n",
    "    f.close()\n",
    "\n",
    "    for word in words:\n",
    "        if word not in stop_word:\n",
    "            returningwords.append(word)\n",
    "    return returningwords\n",
    "\n",
    "def print_dict(dict):\n",
    "    for key in dict.keys():\n",
    "        print(key, dict[key], \" \", end=\"\")\n",
    "    print(\"\\n\", end = \"\")\n",
    "\n",
    "\n",
    "def save_txt(lista, listb, dicta, dictb, dictc):\n",
    "    f = open(\"Q3_Part2.txt\", \"a+\")\n",
    "    f.write(\"Number of unique words in documents: \"+str(lista)+\"\\n\")\n",
    "    f.write(\"Length of documents: \"+str(listb)+\"\\n\")\n",
    "\n",
    "    f.write(\"tf \")\n",
    "    for key in dicta.keys():\n",
    "        f.write(str(key)+\" \"+str(dicta[key])+\"  \")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    f.write(\"idf \")\n",
    "    for key in dictb.keys():\n",
    "        f.write(str(key)+\" \" + str(dictb[key]) + \"  \")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    f.write(\"tf-idf \")\n",
    "    for key in dictc.keys():\n",
    "        f.write(str(key)+\" \" + str(dictc[key]) + \" \")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    f.close()\n",
    "\n",
    "def main():\n",
    "    keyword = [\"statistics\", \"analytics\", \"data\", \"science\"]\n",
    "    urls = get_url()\n",
    "\n",
    "    n_words_list = []\n",
    "    len_document_list = []\n",
    "    tf_dict_all = defaultdict(list)\n",
    "    idf_list = dict([(k, 0) for k in keyword])\n",
    "    tf_idf_dict = {}\n",
    "    for URL in urls:\n",
    "\n",
    "        result_text = get_text(URL, option = \"p\")\n",
    "        result_text.lower()\n",
    "\n",
    "        # STRIPPING\n",
    "        result_text = strip_punctuation_space(result_text)\n",
    "        result_text = result_text.split(' ')\n",
    "        result_text = delete_stop_words(result_text)\n",
    "\n",
    "        # GETTING DATA\n",
    "        len_web = len(result_text)\n",
    "        len_word, _ = make_dict(result_text)\n",
    "        tf_dict = tf(keyword, len_web, result_text)\n",
    "\n",
    "        # ADDING TO LIST\n",
    "        n_words_list.append(len_word)\n",
    "        len_document_list.append(len_web)\n",
    "\n",
    "        # MAKING DICTIONARY OF TF, TF-IDF\n",
    "        for word, tf_value in tf_dict.items():\n",
    "            tf_dict_all[word].append(tf_value)\n",
    "\n",
    "    idf_list = get_idf(tf_dict_all, urls)\n",
    "    tf_idf_dict = cal_tf_idf(tf_dict_all, idf_list, keyword)\n",
    "\n",
    "\n",
    "    print(\"Number of unique words in documents:\", n_words_list)\n",
    "    print(\"Length of documents\", len_document_list)\n",
    "    print(\"tf \", end=\"\")\n",
    "    print_dict(dict(tf_dict_all))\n",
    "    print(\"idf \", end=\"\")\n",
    "    print_dict(idf_list)\n",
    "    print(\"tf-idf \", end=\"\")\n",
    "    print_dict(tf_idf_dict)\n",
    "    save_txt(n_words_list, len_document_list, tf_dict_all, idf_list, tf_idf_dict)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}