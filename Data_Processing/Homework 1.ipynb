{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Homework 1 (Due: Mar. 17, 2021 (11:59 PM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Lim Jeonghan(임정한)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student ID: 2017312172\n",
    "Late submission Days used here: 0 (if there is, please modify here)\n",
    "\n",
    "[Please SUBMIT (1) YOUR IPYNB AND (2) PDF (please use FILE/DOWNLOAD AS/PDF or PRINT PREVIEW/PRINT AS PDF with \"printed output\") TO iCampus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "For this homework you cannot use the python library scikit-learn (sklearn). \n",
    "You can use the python package BeautifulSoup to parse web pages.\n",
    "\n",
    "In this assignment you will retrieve and parse webpages. The text file \"urls.txt\" contains a list of urls for the webpages to be parsed. Each line in the text file corresponds to a url. Use BeautifulSoup to fetch each webpage.\n",
    "\n",
    "Note: For all questions, the words should be converted to lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Q1: Part 1 (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Parse the first webpage document to retrieve the text enclosed in paragraph tags, find the words that end in \"ing\", and count how many times each word appears. \n",
    "\n",
    "Sort these words in decreasing order of frequency, and write the words (along with their corresponding frequencies) in an output file named \"Q1_Part1.txt\". The most frequent word should appear at the top and the least frequent word at the end, and the format of the output file should be:\n",
    "word TAB frequency\n",
    "\n",
    "Example:\n",
    "\n",
    "sorting\t10\n",
    "training\t8\n",
    "broadening\t6\n",
    "extracting\t3\n",
    "evergrowing\t2\n",
    "coming\t1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Q1: Part 2 (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Stop words are natural language words which have very little meaning, such as \"and\", \"the\", \"a\", \"an\", and similar words.\n",
    "\n",
    "Repeat Part 1, but before counting, remove the stop words given in the file \"stop_words.txt\". The ouput for Part 2 should have the same format as Part 1, and should be written to an output file named \"Q1_Part2.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 (10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Again, parse the first webpage document, but this time find and count all outgoing links to other webpages, and write the output to a file named \"Q2.txt\", with each url on a new line.\n",
    "\n",
    "The format of the output file should: number of outgoing urls in the first line, followed by each url on a new line. \n",
    "For example, \n",
    "4\n",
    "https://eng.skku.edu/eng/edu/education.do\n",
    "https://eng.skku.edu/eng/Research/industry/researchStory.do\n",
    "https://eng.skku.edu/eng/Univ-Industry/Research-Business-Found/FactsandFigures.do\n",
    "https://eng.skku.edu/eng/CampusLife/support/employment.do\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Q3: Part 1 (10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "1. Retrieve and parse multiple web pages. The text file \"urls.txt\" contains a list of webpages to be parsed. Each line in the text file corresponds to a url. Use BeautifulSoup to fetch each webpage and parse it as specified below. \n",
    "\n",
    "2. For each webpage document do the following:\n",
    "    1. Retrieve all text enclosed in paragraph tags. \n",
    "    2. Convert the text to lowercase. \n",
    "    3. Strip out punctuation. Note: if you use translate() with string.punctuation, then it may not strip out all characters. Use a regular expression involving \\W to strip out all non alpha-numeric characters.\n",
    "    4. Tokenize into words based on whitespace separation.\n",
    "\n",
    "3. Find the number of unique words in each webpage document. \n",
    "\n",
    "4. Find the Length of each webpage document. The length of a document is defined as the total number of words in the document (not just unique words).\n",
    "\n",
    "5. For each of the following words: “statistics”, “analytics”, “data”, and “science”, \n",
    "    a. Find Term Frequency (tf). \n",
    "    The term frequency (tf) of a term (word) is defined as the number of times that term t occurs in document d, \n",
    "    divided by the total number of words in the document. \n",
    "    The tf of a word depends on the document under consideration. \n",
    "    \n",
    "    b. Find Inverse Document Frequency (idf).\n",
    "    The inverse document frequency of a word is the logarithmically scaled inverse fraction of the documents that \n",
    "    contain the word, obtained by dividing the total number of documents by the number of documents containing the \n",
    "    term, and then taking the logarithm of that ratio.\n",
    "    The idf of a word doesn't depend on any documnet in which the word is present. \n",
    "    To calculate the idf, you will have to use the log function. The base for the log function must be e.\n",
    "    \n",
    "    c. Find tf-idf. \n",
    "    The tf-idf of a word is the product of the term frequency of the word in document d, and its inverse document \n",
    "    frequency. \n",
    "    The tf-idf of a word depends on the document under consideration. \n",
    "    \n",
    "    Reference: https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "    \n",
    "The output should be written to an output file named \"Q3_Part1.txt\".\n",
    "\n",
    "The format of the output file is as shown below:\n",
    "\n",
    "1. Number of unique words in documents: [702, 723, 280]\n",
    "\n",
    "2. Length of documents: [1711, 1928, 563]\n",
    "\n",
    "3. tf\n",
    "    statistics: [0.0070134424313267095, 0.0025933609958506223, 0.0]\n",
    "    analytics: [0.0029222676797194622, 0.0031120331950207467, 0.0]\n",
    "    data: [0.056107539450613676, 0.05446058091286307, 0.0]\n",
    "    science: [0.03798947983635301, 0.011410788381742738, 0.028419182948490232]\n",
    "    \n",
    "4. idf \n",
    "    statistics: 0.510825623766\n",
    "    analytics: 0.510825623766\n",
    "    data: 0.223143551314\n",
    "    science: 0.0\n",
    "    \n",
    "5. tf-idf\n",
    "    statistics: [0.0028437061936282715, 0.0010515173965460695, 0.0]\n",
    "    analytics: [0.0011848775806784465, 0.0012618208758552834, 0.0]\n",
    "    data: [0.022749649549026172, 0.022081865327467459, 0.0]\n",
    "    science: [0.0, 0.0, 0.0]\n",
    "   \n",
    "The above values are for the first three webpage urls given in the file \"urls.txt\". The number of unique words in documents, average length of documents, tf and tf-idf values for the four words, must be in the order of the urls given in \"urls.txt\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Q3: Part 2 (10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Repeat Part 1, but first remove the stop words given in the file \"stop_words.txt\".  \n",
    "The ouput for Part 2 should have the same format as Part 1, and should be written to an output file named \"Q3_Part2.txt\".\n",
    "Note: The length of document, in this case, will not include stop words. Similarly, the number of unique words in documents, and the calculation of tf, idf, tf-idf should be done after removing the stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Sample output for the file \"urls.txt\"\n",
    "\n",
    "1. Number of unique words in documents: [596, 600, 231]\n",
    "\n",
    "2. Length of documents: [1020, 1079, 357]  \n",
    "\n",
    "3. tf\n",
    "    statistics: [0.011764705882352941, 0.004633920296570899, 0.0]\n",
    "    analytics: [0.004901960784313725, 0.005560704355885079, 0.0]\n",
    "    data: [0.09411764705882353, 0.09731232622798888, 0.0]\n",
    "    science: [0.06372549019607843, 0.020389249304911955, 0.04481792717086835]\n",
    "    \n",
    "4. idf \n",
    "    statistics: 0.405465108108\n",
    "    analytics: 0.405465108108\n",
    "    data: 0.405465108108\n",
    "    science: 0.0\n",
    "    \n",
    "5. tf-idf\n",
    "    statistics: [0.0047701777424489925, 0.0018788929940137366, 0.0]\n",
    "    analytics: [0.0019875740593537469, 0.002254671592816484, 0.0]\n",
    "    data: [0.03816142193959194, 0.039456752874288473, 0.0]\n",
    "    science: [0.0, 0.0, 0.0] \n",
    "      \n",
    "The above values are for the first three webpage urls given in the file \"urls.txt\". The number of unique words in documents, average length of documents, tf and tf-idf values for the four words, must be in the order of the urls given in \"urls.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}